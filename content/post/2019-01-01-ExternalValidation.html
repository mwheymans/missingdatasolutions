---
title: "Steps to externally validate a prediction model"
date: '2021-01-01'
thumbnailImage: /images/external.png
thumbnailImagePosition: left
tags: ["external validation"]
---



<p>External validation means that the performance of a prediction model is studied in a new (external) patient dataset that is not used to develop the model. It is important to use the correct predicted probabilities to evaluate if the external validation of the model is successful!<br />
<!--more--></p>
<div id="prediction-model" class="section level2">
<h2>Prediction model</h2>
<p>We will externally validate the following prediction model:</p>
<pre class="r"><code>library(foreign)
dat.orig &lt;- read.spss(file=&quot;Hipstudy.sav&quot;, to.data.frame=TRUE)

library(rms)
dd &lt;- datadist(dat.orig)
options(datadist=&#39;dd&#39;)

dat.orig$Mobility &lt;- factor(dat.orig$Mobility)
fit.orig &lt;- lrm(Mortality ~ Gender + Mobility + Age + ASA, x = T, y = T, data = dat.orig) 
fit.orig</code></pre>
<pre><code>## Logistic Regression Model
##  
##  lrm(formula = Mortality ~ Gender + Mobility + Age + ASA, data = dat.orig, 
##      x = T, y = T)
##  
##                         Model Likelihood      Discrimination    Rank Discrim.    
##                               Ratio Test             Indexes          Indexes    
##  Obs           426    LR chi2      70.28      R2       0.234    C       0.774    
##   0            333    d.f.             5      R2(5,426)0.142    Dxy     0.548    
##   1             93    Pr(&gt; chi2) &lt;0.0001    R2(5,218.1)0.259    gamma   0.550    
##  max |deriv| 1e-06                            Brier    0.145    tau-a   0.187    
##  
##             Coef    S.E.   Wald Z Pr(&gt;|Z|)
##  Intercept  -9.2172 1.6059 -5.74  &lt;0.0001 
##  Gender      0.4623 0.2968  1.56  0.1194  
##  Mobility=2  0.4999 0.3014  1.66  0.0971  
##  Mobility=3  1.8148 0.5723  3.17  0.0015  
##  Age         0.0711 0.0189  3.75  0.0002  
##  ASA         0.7219 0.2144  3.37  0.0008  
## </code></pre>
<pre class="r"><code>pred.orig &lt;- predict(fit.orig, type=&quot;fitted&quot;)</code></pre>
<div id="roc-curve-and-auc" class="section level3">
<h3>ROC curve and AUC</h3>
<pre class="r"><code>library(pROC)
roc(fit.orig$y, pred.orig, ci=TRUE, plot = TRUE)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<p><img src="/post/2019-01-01-ExternalValidation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre><code>## 
## Call:
## roc.default(response = fit.orig$y, predictor = pred.orig, ci = TRUE,     plot = TRUE)
## 
## Data: pred.orig in 333 controls (fit.orig$y 0) &lt; 93 cases (fit.orig$y 1).
## Area under the curve: 0.7738
## 95% CI: 0.7244-0.8232 (DeLong)</code></pre>
</div>
<div id="calibration-curve-and-hl-test" class="section level3">
<h3>Calibration curve and H&amp;L test</h3>
<pre class="r"><code>group.cut &lt;- quantile(pred.orig, c(seq(0, 1, 0.1)))
group &lt;- cut(pred.orig, group.cut) 
pred.prob &lt;- tapply(pred.orig, group, mean)

# Observed probabilities
obs.prob &lt;- tapply(fit.orig$y, group, mean)
cal.m &lt;- data.frame(cbind(pred.prob, obs.prob))</code></pre>
<pre class="r"><code>library(ggplot2)
plot(ggplot(cal.m,aes(x=pred.prob,y=obs.prob)) + 
  scale_x_continuous(name = &quot;Predicted Probabilities&quot;,
    breaks = seq(0, 1, 0.1), limits=c(0, 0.8)) +
  scale_y_continuous(name = &quot;Observed Probabilities&quot;,
    breaks = seq(0, 1, 0.1), limits=c(0, 0.8)) + 
  geom_smooth(method=&#39;lm&#39;, formula = y~x, se = FALSE) + 
  geom_point(size=2, shape = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype=2) + 
  ggtitle(&quot;Calibration Curve&quot;) )</code></pre>
<p><img src="/post/2019-01-01-ExternalValidation_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>library(ResourceSelection)
hoslem.test(fit.orig$y, pred.orig) # Hosmer and Lemeshow test from package ResourceSelection</code></pre>
<pre><code>## 
## 	Hosmer and Lemeshow goodness of fit (GOF) test
## 
## data:  fit.orig$y, pred.orig
## X-squared = 4.6762, df = 8, p-value = 0.7916</code></pre>
<p>This model has an <strong>C-index (ROC) value of 0.774</strong> and an <strong>R2 of 0.234</strong>. <strong>The Hosmer and Lemeshow test is not significant</strong> (for now we use it as a quick test) indicating that the model calibrates well.</p>
</div>
</div>
<div id="steps-to-externally-validate-a-prediction-model" class="section level2">
<h2>Steps to externally validate a prediction model</h2>
<div id="determine-the-linear-predictor-of-the-model.-this-is-in-our-case" class="section level4">
<h4>1. Determine the Linear Predictor of the model. This is in our case:</h4>
<pre class="r"><code>coef.orig &lt;- coef(fit.orig) 
coef.orig # Coefficients of original model</code></pre>
<pre><code>##   Intercept      Gender  Mobility=2  Mobility=3         Age         ASA 
## -9.21721717  0.46226952  0.49991610  1.81481732  0.07109868  0.72188861</code></pre>
</div>
<div id="transport-the-original-regression-coefficients-to-the-external-dataset-and-calculate-the-linear-predictor." class="section level4">
<h4>2. Transport the original regression coefficients to the external dataset and calculate the linear predictor.</h4>
<pre class="r"><code>fit.ext &lt;- lrm(Mortality ~ Gender + Mobility + Age + ASA, x = T, y = T, data = dat.ext) 
lp.ext &lt;- model.matrix(fit.ext) %*% coef.orig # model.matrix extracts predictor values of external dataset, including dummy coded variables. 

# The same as above. Use for newdata the external dataset
lp.ext &lt;- predict(fit.orig, newdata=dat.ext) </code></pre>
</div>
<div id="determine-the-amount-of-miscalibration-in-intercept-and-calibration-slope." class="section level4">
<h4>3. Determine the amount of miscalibration in intercept and calibration slope.</h4>
<pre class="r"><code>fit.ext.lp &lt;- lrm(fit.ext$y ~ lp.ext)
fit.ext.lp</code></pre>
<pre><code>## Logistic Regression Model
##  
##  lrm(formula = fit.ext$y ~ lp.ext)
##  
##                         Model Likelihood    Discrimination    Rank Discrim.    
##                               Ratio Test           Indexes          Indexes    
##  Obs           300    LR chi2      43.89    R2       0.205    C       0.750    
##   0            230    d.f.             1    R2(1,300)0.133    Dxy     0.499    
##   1             70    Pr(&gt; chi2) &lt;0.0001    R2(1,161)0.234    gamma   0.501    
##  max |deriv| 2e-10                          Brier    0.156    tau-a   0.179    
##  
##            Coef    S.E.   Wald Z Pr(&gt;|Z|)
##  Intercept -0.6594 0.1567 -4.21  &lt;0.0001 
##  lp.ext     0.8767 0.1554  5.64  &lt;0.0001 
## </code></pre>
<p>The coefficient values of -0.6594 and 0.8767 for the intercept and slope respectively
indicate the amount of miscalibration in the intercept ansd slope value.
Without miscalibration these values are 0 and 1 respectively.</p>
</div>
<div id="deterimne-discrimination-and-calibration-in-the-external-dataset" class="section level4">
<h4>4. Deterimne Discrimination and Calibration in the external dataset</h4>
</div>
<div id="discrimination" class="section level4">
<h4>Discrimination</h4>
<pre class="r"><code>pred.ext &lt;-  1/(1+exp(-lp.ext))

roc(fit.ext$y, pred.ext, ci=TRUE, plot = TRUE)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<p><img src="/post/2019-01-01-ExternalValidation_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre><code>## 
## Call:
## roc.default(response = fit.ext$y, predictor = pred.ext, ci = TRUE,     plot = TRUE)
## 
## Data: pred.ext in 230 controls (fit.ext$y 0) &lt; 70 cases (fit.ext$y 1).
## Area under the curve: 0.7496
## 95% CI: 0.6893-0.8098 (DeLong)</code></pre>
<p>The AUC drops from 0.774 to 0.7496.</p>
</div>
<div id="calibration" class="section level4">
<h4>Calibration</h4>
<pre class="r"><code># Predicted probabilities
pred.ext &lt;-  1/(1+exp(-lp.ext))
group.cut &lt;- quantile(pred.ext, c(seq(0, 1, 0.1)))
group &lt;- cut(pred.ext, group.cut) 
pred.prob &lt;- tapply(pred.ext, group, mean)

# Observed probabilities
obs.prob &lt;- tapply(fit.ext$y, group, mean)
cal.m.ext &lt;- data.frame(cbind(pred.prob, obs.prob))</code></pre>
<pre class="r"><code>library(ggplot2)
plot(ggplot(cal.m.ext,aes(x=pred.prob,y=obs.prob)) + 
  scale_x_continuous(name = &quot;Predicted Probabilities&quot;,
    breaks = seq(0, 1, 0.1), limits=c(0, 0.8)) +
  scale_y_continuous(name = &quot;Observed Probabilities&quot;,
    breaks = seq(0, 1, 0.1), limits=c(0, 0.8)) + 
  geom_smooth(method=&quot;lm&quot;, formula = y~x, se = FALSE) + 
  geom_point(size=2, shape = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype=2) + 
  ggtitle(&quot;Calibration Curve&quot;) )</code></pre>
<p><img src="/post/2019-01-01-ExternalValidation_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The calibration curve deviates from the ideal line.</p>
</div>
<div id="hosmer-and-lemeshow-test" class="section level4">
<h4>Hosmer and Lemeshow test</h4>
<pre class="r"><code>hoslem.test(fit.ext$y, pred.ext)</code></pre>
<pre><code>## 
## 	Hosmer and Lemeshow goodness of fit (GOF) test
## 
## data:  fit.ext$y, pred.ext
## X-squared = 23.991, df = 8, p-value = 0.0023</code></pre>
<p>The Hosmer and Lemeshow test is now significant. The test confirms
the miscalibration in the intercept and calibration slope, visualized
on the calibration curve.</p>
</div>
<div id="incorrect-external-validation-recalibrated-predictions" class="section level3">
<h3>Incorrect external validation: recalibrated predictions</h3>
<p>When you externally validate a prediction model you have to use<br />
<em>uncalibrated</em> predictions. Otherwise the external validation
procedure is incorrect.</p>
<p>The <em>correct</em> uncalibrated predictions were used in the calibration curve above and were
calculated “outside” the model to detect miscalibration by using the following formula
to calculate predicted probabilities:</p>
<pre class="r"><code>pred.ext &lt;-  1/(1+exp(-lp.ext))</code></pre>
<p><em>Incorrect</em> Calibrated predictions are calculated via the prediction model, when you use:</p>
<pre class="r"><code>fit.ext.lp &lt;- lrm(fit.ext$y ~ lp.ext) # the coefficient values reflect the amount of miscalibration 
fit.ext.lp</code></pre>
<pre><code>## Logistic Regression Model
##  
##  lrm(formula = fit.ext$y ~ lp.ext)
##  
##                         Model Likelihood    Discrimination    Rank Discrim.    
##                               Ratio Test           Indexes          Indexes    
##  Obs           300    LR chi2      43.89    R2       0.205    C       0.750    
##   0            230    d.f.             1    R2(1,300)0.133    Dxy     0.499    
##   1             70    Pr(&gt; chi2) &lt;0.0001    R2(1,161)0.234    gamma   0.501    
##  max |deriv| 2e-10                          Brier    0.156    tau-a   0.179    
##  
##            Coef    S.E.   Wald Z Pr(&gt;|Z|)
##  Intercept -0.6594 0.1567 -4.21  &lt;0.0001 
##  lp.ext     0.8767 0.1554  5.64  &lt;0.0001 
## </code></pre>
<pre class="r"><code>pred.ext.recal &lt;- predict(fit.ext.lp, type=&quot;fitted&quot;)</code></pre>
<p>These predictions are calibrated in the model by using the intercept and slope
coefficient values as correction factors. These values are -0.6594 and 0.8631 for the intercept
and slope respectively (equal to the values of step 3). Without miscalibration these intercept and
slope coefficient values would be 0 and 1. These recalibrated predictions lead to the following results
for discrimination and calibration.</p>
<div id="discrimination-1" class="section level4">
<h4>Discrimination</h4>
<pre class="r"><code>roc(fit.ext$y, pred.ext.recal, ci=TRUE, plot = TRUE)</code></pre>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<p><img src="/post/2019-01-01-ExternalValidation_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre><code>## 
## Call:
## roc.default(response = fit.ext$y, predictor = pred.ext.recal,     ci = TRUE, plot = TRUE)
## 
## Data: pred.ext.recal in 230 controls (fit.ext$y 0) &lt; 70 cases (fit.ext$y 1).
## Area under the curve: 0.7496
## 95% CI: 0.6893-0.8098 (DeLong)</code></pre>
</div>
<div id="calibration-1" class="section level4">
<h4>Calibration</h4>
<pre class="r"><code>library(ggplot2)
cal.plot &lt;- ggplot(cal.m.ext,aes(x=pred.prob,y=obs.prob)) + 
  scale_x_continuous(name = &quot;Predicted Probabilities&quot;,
    breaks = seq(0, 1, 0.1), limits=c(0, 0.8)) +
  scale_y_continuous(name = &quot;Observed Probabilities&quot;,
    breaks = seq(0, 1, 0.1), limits=c(0, 0.8)) + 
  geom_smooth(method=&quot;lm&quot;, formula = y~x, se = FALSE) + 
  geom_point(size=2, shape = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype=2) + 
  ggtitle(&quot;Calibration Curve&quot;) 
plot(cal.plot)</code></pre>
<p><img src="/post/2019-01-01-ExternalValidation_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="hosmer-and-lemeshow-test-1" class="section level4">
<h4>Hosmer and Lemeshow test</h4>
<pre class="r"><code>hoslem.test(fit.ext$y, pred.ext.recal)</code></pre>
<pre><code>## 
## 	Hosmer and Lemeshow goodness of fit (GOF) test
## 
## data:  fit.ext$y, pred.ext.recal
## X-squared = 5.9383, df = 8, p-value = 0.6541</code></pre>
<p>The Hosmer and Lemeshow test is not significant and the calibration
curve looks as if the model calibrates well in the external dataset.
This is <em>incorrect</em> and the result of working with calibrated predictions.
As you may have noticed, these calibrated predictions do not influence
the ROC curve and AUC.</p>
</div>
<div id="for-external-validation-in-multiple-imputed-datasets-see-httpmissingdatasolutions.rbind.iopsfmi." class="section level4">
<h4>For external validation in Multiple Imputed datasets see <a href="http://missingdatasolutions.rbind.io/psfmi/" class="uri">http://missingdatasolutions.rbind.io/psfmi/</a>.</h4>
</div>
</div>
</div>
